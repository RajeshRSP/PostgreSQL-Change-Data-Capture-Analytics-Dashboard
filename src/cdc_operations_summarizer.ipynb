{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8083456e-1459-4c9d-b068-0131072b4ecb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T05:20:33.574644Z",
     "iopub.status.busy": "2024-04-09T05:20:33.574085Z",
     "iopub.status.idle": "2024-04-09T05:20:33.578423Z",
     "shell.execute_reply": "2024-04-09T05:20:33.577905Z",
     "shell.execute_reply.started": "2024-04-09T05:20:33.574612Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DateType\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "import os\n",
    "from pyspark.sql.functions import col, to_date, sum as spark_sum, concat_ws\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "import time\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "461fc055-6e7f-47c3-9a09-a708c126c901",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T05:20:35.808551Z",
     "iopub.status.busy": "2024-04-09T05:20:35.808206Z",
     "iopub.status.idle": "2024-04-09T05:20:36.295580Z",
     "shell.execute_reply": "2024-04-09T05:20:36.294904Z",
     "shell.execute_reply.started": "2024-04-09T05:20:35.808526Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "def initialize_spark_session():\n",
    "    \"\"\"\n",
    "    Initializes a SparkSession.\n",
    "\n",
    "    Returns:\n",
    "        SparkSession: Initialized SparkSession object.\n",
    "    \"\"\"\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Read Parquet Files\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Define schema for Parquet files\n",
    "def define_schema():\n",
    "    \"\"\"\n",
    "    Defines the schema for Parquet files.\n",
    "\n",
    "    Returns:\n",
    "        StructType: Defined schema for Parquet files.\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"timestamp\", TimestampType(), True),\n",
    "        StructField(\"date\", DateType(), True),\n",
    "        StructField(\"year\", IntegerType(), True),\n",
    "        StructField(\"month\", IntegerType(), True),\n",
    "        StructField(\"day\", IntegerType(), True),\n",
    "        StructField(\"hour\", IntegerType(), True),\n",
    "        StructField(\"minute\", IntegerType(), True),\n",
    "        StructField(\"schema_name\", StringType(), True),\n",
    "        StructField(\"table_name\", StringType(), True),\n",
    "        StructField(\"insert\", IntegerType(), True),\n",
    "        StructField(\"update\", IntegerType(), True),\n",
    "        StructField(\"truncate\", IntegerType(), True),\n",
    "        StructField(\"delete\", IntegerType(), True),\n",
    "    ])\n",
    "\n",
    "# Read Parquet files from given base path\n",
    "def read_parquet_files(base_path):\n",
    "    \"\"\"\n",
    "    Reads Parquet files from the given base path.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): Base path where Parquet files are located.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Combined DataFrame containing data from all Parquet files.\n",
    "    \"\"\"\n",
    "    subfolders = [folder for folder in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, folder))]\n",
    "    combined_df = spark.createDataFrame([], schema)\n",
    "\n",
    "    for subfolder in subfolders:\n",
    "        file_path = os.path.join(base_path, subfolder)\n",
    "        df = spark.read.schema(schema).parquet(file_path)\n",
    "        combined_df = combined_df.union(df)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Group combined DataFrame by date, hour, schema_name, and table_name\n",
    "def group_by_date_hour(combined_df):\n",
    "    \"\"\"\n",
    "    Groups combined DataFrame by date, hour, schema_name, and table_name.\n",
    "\n",
    "    Args:\n",
    "        combined_df (DataFrame): Combined DataFrame containing data from Parquet files.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Grouped DataFrame with aggregated operations counts.\n",
    "    \"\"\"\n",
    "    grouped_by_hour = combined_df.groupBy('date','hour', 'schema_name', 'table_name') \\\n",
    "        .agg(spark_sum('insert').alias('insert_sum'),\n",
    "             spark_sum('update').alias('update_sum'),\n",
    "             spark_sum('truncate').alias('truncate_sum'),\n",
    "             spark_sum('delete').alias('delete_sum'))\n",
    "\n",
    "    grouped_with_ops = grouped_by_hour.withColumn('Total operations', \n",
    "                                                  col('insert_sum') + col('update_sum') + col('truncate_sum') + col('delete_sum'))\n",
    "    grouped_with_ops = grouped_with_ops.select('date', 'hour', 'schema_name', 'table_name', \n",
    "                                               'insert_sum', 'update_sum', 'truncate_sum', 'delete_sum', 'Total operations')\n",
    "    return grouped_with_ops.orderBy(grouped_with_ops['date'].desc(), grouped_with_ops['hour'].desc(),grouped_with_ops['Total operations'].desc())\n",
    "\n",
    "# Get top 3 tables based on total operations\n",
    "def get_top_3_tables(grouped_with_ops):\n",
    "    \"\"\"\n",
    "    Gets the top 3 tables based on total operations.\n",
    "\n",
    "    Args:\n",
    "        grouped_with_ops (DataFrame): DataFrame with aggregated operations counts.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame containing top 3 tables based on total operations.\n",
    "    \"\"\"\n",
    "    window_spec = Window.partitionBy('date', 'hour').orderBy(col('Total operations').desc())\n",
    "    ranked_df = grouped_with_ops.withColumn('rank', row_number().over(window_spec))\n",
    "    return ranked_df.filter(col('rank') <= 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5662d69a-fccd-49b3-8fbf-4392cac227e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T05:20:38.485876Z",
     "iopub.status.busy": "2024-04-09T05:20:38.485512Z",
     "iopub.status.idle": "2024-04-09T05:22:10.955343Z",
     "shell.execute_reply": "2024-04-09T05:22:10.954643Z",
     "shell.execute_reply.started": "2024-04-09T05:20:38.485852Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the base path where Parquet files are located\n",
    "base_path = '/opt/rpx/repos/jupyter/notebook/parquet'\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = initialize_spark_session()\n",
    "\n",
    "# Define schema for Parquet files\n",
    "schema = define_schema()\n",
    "\n",
    "# Read Parquet files from the base path and combine into a DataFrame\n",
    "combined_df = read_parquet_files(base_path)\n",
    "\n",
    "# Print the schema of the combined DataFrame\n",
    "combined_df.printSchema()\n",
    "\n",
    "# Group the combined DataFrame by date, hour, schema_name, and table_name and display the results\n",
    "grouped_with_ops = group_by_date_hour(combined_df)\n",
    "grouped_with_ops.show(1000)\n",
    "\n",
    "# Print the count of rows and number of columns in the grouped DataFrame\n",
    "print((grouped_with_ops.count(), len(grouped_with_ops.columns)))\n",
    "\n",
    "# Calculate the sum of values in the \"Total operations\" column and print the result\n",
    "column_sum = grouped_with_ops.select(spark_sum(\"Total operations\")).collect()[0][0]\n",
    "print(\"Sum of values in the column:\", column_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a17fb38e-f33a-411d-8c4c-8d3ae0b859c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T08:01:43.457588Z",
     "iopub.status.busy": "2024-04-05T08:01:43.456981Z",
     "iopub.status.idle": "2024-04-05T08:01:47.283981Z",
     "shell.execute_reply": "2024-04-05T08:01:47.283204Z",
     "shell.execute_reply.started": "2024-04-05T08:01:43.457558Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----------+--------------------+----------+----------+------------+----------+----------------+----+\n",
      "|      date|hour|schema_name|          table_name|insert_sum|update_sum|truncate_sum|delete_sum|Total operations|rank|\n",
      "+----------+----+-----------+--------------------+----------+----------+------------+----------+----------------+----+\n",
      "|2024-04-05|   7|        ldc|lit_docs_ptab_pet...|    640772|         0|           0|    640772|         1281544|   1|\n",
      "|2024-04-05|   7|       docs|                docs|    615111|         0|           0|    615111|         1230222|   2|\n",
      "|2024-04-05|   7|        ldc|lit_docs_patent_i...|    464625|         0|           0|    464625|          929250|   3|\n",
      "+----------+----+-----------+--------------------+----------+----------+------------+----------+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top3_df = get_top_3_tables(grouped_with_ops)\n",
    "top3_df.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
